{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T09:20:42.852777Z","iopub.status.busy":"2024-06-24T09:20:42.852136Z","iopub.status.idle":"2024-06-24T09:21:47.120751Z","shell.execute_reply":"2024-06-24T09:21:47.119832Z","shell.execute_reply.started":"2024-06-24T09:20:42.852749Z"},"id":"3Lb2rO_4XsZU","trusted":true},"outputs":[],"source":["!pip install sentence-transformers transformers faiss-gpu pymupdf nltk datasets faiss-cpu langdetect"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T09:21:47.123694Z","iopub.status.busy":"2024-06-24T09:21:47.122994Z","iopub.status.idle":"2024-06-24T09:22:11.139505Z","shell.execute_reply":"2024-06-24T09:22:11.138661Z","shell.execute_reply.started":"2024-06-24T09:21:47.123655Z"},"id":"7pnbkQYcX88T","trusted":true},"outputs":[],"source":["import os\n","from sentence_transformers import SentenceTransformer\n","from transformers import pipeline, T5ForConditionalGeneration, T5Tokenizer\n","import torch\n","import faiss\n","import gc\n","import numpy as np\n","import fitz\n","import re\n","import unicodedata\n","import spacy\n","import nltk\n","import json\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from langdetect import detect, LangDetectException"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T09:22:11.141212Z","iopub.status.busy":"2024-06-24T09:22:11.140652Z","iopub.status.idle":"2024-06-24T09:22:11.151278Z","shell.execute_reply":"2024-06-24T09:22:11.150269Z","shell.execute_reply.started":"2024-06-24T09:22:11.141185Z"},"id":"_TzRmZu7X-9Q","trusted":true},"outputs":[],"source":["class PDFValidator:\n","    def __init__(self):\n","        self.nlp = spacy.load(\"en_core_web_sm\")  # Load English model for spaCy\n","\n","    def is_valid_pdf(self, pdf_path):\n","        # Check if the file is a valid PDF and not empty\n","        try:\n","            document = fitz.open(pdf_path)  # Open the PDF file\n","            if document.page_count == 0:\n","                print(f\"PDF is empty: {pdf_path}\")\n","                return False\n","            text = \"\"\n","            for page_num in range(document.page_count):\n","                page = document.load_page(page_num)  # Load each page\n","                text += page.get_text()  # Extract text from the page\n","            document.close()  # Close the PDF file\n","            if not text.strip():\n","                print(f\"PDF has no extractable text: {pdf_path}\")\n","                return False\n","            if not self.is_english(text):\n","                print(f\"PDF is not in English: {pdf_path}\")\n","                return False\n","            return True  # PDF is valid if it passes all checks\n","        except Exception as e:\n","            print(f\"Error opening PDF: {e}\")\n","            return False\n","\n","    def is_english(self, text):\n","        # Check if the text is in English\n","        try:\n","            lang = detect(text)  # Detect the language of the text\n","            return lang == 'en'  # Return True if language is English ('en')\n","        except LangDetectException:\n","            return False  # Return False if an error occurs during language detection"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T09:22:11.154342Z","iopub.status.busy":"2024-06-24T09:22:11.153984Z","iopub.status.idle":"2024-06-24T09:22:11.587444Z","shell.execute_reply":"2024-06-24T09:22:11.586400Z","shell.execute_reply.started":"2024-06-24T09:22:11.154309Z"},"id":"NNxpO-bYaEbK","trusted":true},"outputs":[],"source":["class QuestionValidator:\n","    @staticmethod\n","    def is_valid_question(question):\n","        # Validate the question\n","        if not question.strip() or question.isdigit():\n","            return False  # Return False if the question is empty or consists only of digits\n","        return True  # Return True if the question is valid\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T09:22:11.589114Z","iopub.status.busy":"2024-06-24T09:22:11.588838Z","iopub.status.idle":"2024-06-24T09:22:11.617982Z","shell.execute_reply":"2024-06-24T09:22:11.617070Z","shell.execute_reply.started":"2024-06-24T09:22:11.589089Z"},"id":"WlJAGWJ4YD2p","trusted":true},"outputs":[],"source":["class PDFQASystem:\n","    def __init__(self, t5_model_name='google/flan-t5-xl', sentence_transformer_model_name='sentence-transformers/multi-qa-mpnet-base-dot-v1', device='cuda'):\n","        \"\"\"\n","        Initialize the PDFQASystem with specified models and device.\n","\n","        Args:\n","        - t5_model_name (str): Name or path of the T5 model to use for question answering.\n","        - sentence_transformer_model_name (str): Name or path of the Sentence Transformer model for sentence embeddings.\n","        - device (str): Device to run the models on ('cuda' for GPU, 'cpu' for CPU).\n","\n","        Raises:\n","        - Exception: If there's an error loading the specified models.\n","        \"\"\"\n","        self.device = device\n","        self.sentence_transformer_model_name = sentence_transformer_model_name\n","        self.nlp = spacy.load(\"en_core_web_sm\")  # Load spaCy English model\n","\n","        # Load T5 model and tokenizer, and Sentence Transformer model\n","        try:\n","            self.t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_name).to(device)\n","            self.tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n","            self.sentence_model = SentenceTransformer(sentence_transformer_model_name).to(device)\n","        except Exception as e:\n","            print(f\"Error loading models: {e}\")\n","            raise\n","\n","        self.embeddings = None  # Placeholder for embeddings\n","        self.index = None  # Placeholder for Faiss index\n","        self.chunks = []  # List to store text chunks extracted from PDFs\n","\n","    def extract_text_from_pdf(self, pdf_path):\n","        \"\"\"\n","        Extract text content from a PDF file.\n","\n","        Args:\n","        - pdf_path (str): Path to the PDF file.\n","\n","        Returns:\n","        - str: Extracted text content from the PDF.\n","\n","        Raises:\n","        - Exception: If there's an error extracting text from the PDF.\n","        \"\"\"\n","        try:\n","            document = fitz.open(pdf_path)  # Open the PDF file\n","            text = \"\"\n","            for page_num in range(document.page_count):\n","                page = document.load_page(page_num)  # Load each page\n","                text += page.get_text()  # Extract text from the page\n","            document.close()  # Close the PDF file\n","            return text\n","        except Exception as e:\n","            print(f\"Error extracting text from PDF: {e}\")\n","            raise\n","\n","    def clean_text(self, text):\n","        \"\"\"\n","        Clean and normalize text.\n","\n","        Args:\n","        - text (str): Text to clean and normalize.\n","\n","        Returns:\n","        - str: Cleaned and normalized text.\n","        \"\"\"\n","        text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with single space\n","        text = text.lower()  # Convert text to lowercase\n","        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')  # Remove accents\n","        return text\n","\n","    def chunk_text_by_tokens(self, document, max_token_limit=128):\n","        \"\"\"\n","        Chunk the document into segments based on token length.\n","\n","        Args:\n","        - document (str): Text document to chunk.\n","        - max_token_limit (int): Maximum token length per chunk.\n","\n","        Returns:\n","        - list: List of text chunks.\n","        \"\"\"\n","        text = self.nlp(document)  # Process text with spaCy\n","        sentences = list(text.sents)  # Split text into sentences\n","\n","        chunks = []\n","        current_chunk = []\n","        current_length = 0\n","\n","        for sentence in sentences:\n","            sentence_text = str(sentence).strip()\n","            tokens = self.tokenizer.encode(sentence_text)  # Tokenize sentence\n","            sentence_length = len(tokens)\n","\n","            if current_length + sentence_length > max_token_limit:\n","                chunks.append(' '.join(current_chunk))\n","                current_chunk = [sentence_text]\n","                current_length = sentence_length\n","            else:\n","                current_chunk.append(sentence_text.strip())\n","                current_length += sentence_length\n","\n","        if current_chunk:\n","            chunks.append(' '.join(current_chunk))\n","\n","        return chunks\n","\n","    def generate_embeddings(self, chunks):\n","        \"\"\"\n","        Generate embeddings for text chunks using Sentence Transformer model.\n","\n","        Args:\n","        - chunks (list): List of text chunks.\n","\n","        Returns:\n","        - torch.Tensor: Tensor of embeddings for all chunks.\n","        \"\"\"\n","        embeddings = []\n","\n","        for i in range(0, len(chunks), 10):  # Process chunks in batches of 10\n","            batch = chunks[i:i+10]\n","            batch_embeddings = self.sentence_model.encode(batch, convert_to_tensor=True, show_progress_bar=True)  # Encode chunks into embeddings\n","            embeddings.append(batch_embeddings.cpu().numpy())  # Append batch embeddings to list\n","            del batch_embeddings  # Delete batch embeddings to free memory\n","            torch.cuda.empty_cache()  # Clear GPU memory cache\n","            gc.collect()  # Perform garbage collection\n","\n","        return torch.tensor(np.vstack(embeddings))  # Stack embeddings and convert to PyTorch tensor\n","\n","    def setup_vector_database(self, embeddings):\n","        \"\"\"\n","        Setup Faiss vector database for efficient similarity search.\n","\n","        Args:\n","        - embeddings (torch.Tensor): Tensor of embeddings.\n","\n","        Returns:\n","        - faiss.IndexFlatL2: Faiss index for embeddings.\n","        \"\"\"\n","        dimension = embeddings.shape[1]  # Get embedding dimension\n","        index = faiss.IndexFlatL2(dimension)  # Initialize Faiss index\n","        index.add(embeddings.cpu().numpy())  # Add embeddings to Faiss index\n","        return index\n","\n","    def get_query_embedding(self, query):\n","        \"\"\"\n","        Generate embedding for a query using Sentence Transformer model.\n","\n","        Args:\n","        - query (str): Query string.\n","\n","        Returns:\n","        - torch.Tensor: Embedding for the query.\n","        \"\"\"\n","        query_embedding = self.sentence_model.encode([query], convert_to_tensor=True, show_progress_bar=True)  # Encode query into embedding\n","        return query_embedding\n","\n","    def search_relevant_chunks(self, query_embedding, top_k=3):\n","        \"\"\"\n","        Search for relevant chunks based on query embedding.\n","\n","        Args:\n","        - query_embedding (torch.Tensor): Embedding for the query.\n","        - top_k (int): Number of top chunks to retrieve.\n","\n","        Returns:\n","        - list: List of relevant chunks.\n","        \"\"\"\n","        scores, indices = self.index.search(query_embedding.cpu().numpy(), top_k)  # Perform similarity search using Faiss index\n","        relevant_chunks = [self.chunks[idx] for idx in indices[0]]  # Get relevant chunks based on indices\n","        return relevant_chunks\n","\n","    def generate_answer(self, query, relevant_chunks):\n","        \"\"\"\n","        Generate an answer to the query based on relevant chunks.\n","\n","        Args:\n","        - query (str): Query string.\n","        - relevant_chunks (list): List of relevant text chunks.\n","\n","        Returns:\n","        - str: Generated answer to the query.\n","        \"\"\"\n","        prompt = f\"Question: {query}\\nContext:\\n\"  # Define prompt for T5 model\n","        for chunk in relevant_chunks:\n","            prompt += f\"{chunk}\\n\"  # Add relevant chunks to context in prompt\n","        prompt += \"Answer:\"\n","        input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)  # Tokenize and encode prompt\n","        output = self.t5_model.generate(input_ids, max_length=512, early_stopping=False, num_beams=5, no_repeat_ngram_size=2)  # Generate answer using T5 model\n","        answer = self.tokenizer.decode(output[0], skip_special_tokens=True)  # Decode generated answer\n","        return answer\n","\n","    def process_documents(self, pdf_paths):\n","        \"\"\"\n","        Process multiple PDF documents to extract text, clean, chunk, generate embeddings, and setup index.\n","\n","        Args:\n","        - pdf_paths (list): List of paths to PDF documents.\n","        \"\"\"\n","        all_chunks = []\n","        for pdf_path in pdf_paths:\n","            text = self.extract_text_from_pdf(pdf_path)  # Extract text from PDF\n","            text = self.clean_text(text)  # Clean and normalize text\n","            chunks = self.chunk_text_by_tokens(text)  # Chunk text into segments\n","            all_chunks.extend(chunks)  # Extend list of all chunks\n","\n","        print('Total number of chunks:', len(all_chunks))\n","\n","        self.chunks = all_chunks  # Store all chunks\n","        self.embeddings = self.generate_embeddings(all_chunks)  # Generate embeddings for all chunks\n","        self.index = self.setup_vector_database(self.embeddings)  # Setup Faiss index for embeddings\n","\n","    def ask_question(self, query, top_k=3):\n","        \"\"\"\n","        Process a question to find relevant chunks, generate an answer, and return the answer.\n","\n","        Args:\n","        - query (str): Question to answer.\n","        - top_k (int): Number of top relevant chunks to consider.\n","\n","        Returns:\n","        - str: Generated answer to the question.\n","        \"\"\"\n","        query_embedding = self.get_query_embedding(query)  # Generate embedding for the query\n","        relevant_chunks = self.search_relevant_chunks(query_embedding, top_k)  # Search for relevant chunks\n","        print('Relevant Chunks:', relevant_chunks)\n","\n","        answer = self.generate_answer(query, relevant_chunks)  # Generate answer based on relevant chunks\n","        print(f\"Answer: {answer}\")\n","        return answer"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T09:22:11.619427Z","iopub.status.busy":"2024-06-24T09:22:11.619131Z","iopub.status.idle":"2024-06-24T09:22:11.634326Z","shell.execute_reply":"2024-06-24T09:22:11.633351Z","shell.execute_reply.started":"2024-06-24T09:22:11.619403Z"},"id":"t57jJBxNZnbt","trusted":true},"outputs":[],"source":["import json  # Import JSON module for JSON parsing\n","\n","class SimpleParser:\n","    @staticmethod\n","    def parse(raw_answer):\n","        \"\"\"\n","        Static method to parse a raw answer by stripping leading and trailing whitespace.\n","\n","        Args:\n","        - raw_answer (str): Raw answer string.\n","\n","        Returns:\n","        - str: Parsed answer with stripped whitespace.\n","        \"\"\"\n","        return raw_answer.strip()\n","\n","class JSONParser:\n","    @staticmethod\n","    def parse(raw_answer):\n","        \"\"\"\n","        Static method to parse a raw answer into JSON format.\n","\n","        Args:\n","        - raw_answer (str): Raw answer string.\n","\n","        Returns:\n","        - str: JSON-encoded answer with {\"answer\": raw_answer} format.\n","        \"\"\"\n","        return json.dumps({\"answer\": raw_answer.strip()})\n","\n","class ResponseParser:\n","    def __init__(self, parser_type):\n","        \"\"\"\n","        Initialize ResponseParser with a specified parser type.\n","\n","        Args:\n","        - parser_type (str): Type of parser to use ('simple' or 'json').\n","\n","        Raises:\n","        - ValueError: If an invalid parser type is provided.\n","        \"\"\"\n","        if parser_type == 'simple':\n","            self.parser = SimpleParser()  # Initialize SimpleParser instance\n","        elif parser_type == 'json':\n","            self.parser = JSONParser()  # Initialize JSONParser instance\n","        else:\n","            raise ValueError(\"Invalid parser type. Supported types: 'simple', 'json'\")\n","\n","    def parse_answer(self, raw_answer):\n","        \"\"\"\n","        Parse a raw answer using the selected parser.\n","\n","        Args:\n","        - raw_answer (str): Raw answer string.\n","\n","        Returns:\n","        - str: Parsed answer based on the selected parser's logic.\n","        \"\"\"\n","        return self.parser.parse(raw_answer)  # Delegate parsing to the selected parser\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-06-24T09:22:11.635997Z","iopub.status.busy":"2024-06-24T09:22:11.635659Z"},"id":"y_LR426yZy3k","trusted":true},"outputs":[],"source":["def main():\n","    pdf_paths = []\n","    qa_system = PDFQASystem()  # Initialize PDF Question Answering System\n","    pdf_validator = PDFValidator()  # Initialize PDF validator for checking PDF validity\n","    question_validator = QuestionValidator()  # Initialize question validator for checking question validity\n","\n","    parser_type = input(\"Enter parser type ('simple' or 'json'): \").lower()  # Prompt user to choose parser type\n","    response_parser = ResponseParser(parser_type)  # Initialize response parser based on user's choice\n","\n","    # Collect paths of PDFs from user input until 'done' is entered\n","    while True:\n","        pdf_path = input(\"Enter PDF path (or 'done' to finish): \")\n","        if pdf_path.lower() == 'done':\n","            break\n","        pdf_paths.append(pdf_path)\n","\n","    valid_pdf_paths = []\n","    # Validate each PDF path and filter out invalid ones\n","    for path in pdf_paths:\n","        if not pdf_validator.is_valid_pdf(path):\n","            print(f\"Invalid PDF: {path}\")\n","        else:\n","            valid_pdf_paths.append(path)\n","\n","    if valid_pdf_paths:\n","        qa_system.process_documents(valid_pdf_paths)  # Process valid PDFs to extract information\n","    else:\n","        print(\"No valid PDFs to process.\")\n","\n","    # Prompt user to ask questions until 'exit' is entered\n","    while True:\n","        question = input(\"Enter your question (or 'exit' to quit): \")\n","        if question.lower() == 'exit':\n","            break\n","\n","        if not question_validator.is_valid_question(question):\n","            print(\"Invalid question. Please enter a valid non-empty string question.\")\n","            continue\n","\n","        raw_answer = qa_system.ask_question(question)  # Get raw answer from QA system\n","        parsed_answer = response_parser.parse_answer(raw_answer)  # Parse raw answer based on chosen parser\n","        print(parsed_answer)  # Display parsed answer to the user\n","\n","if __name__ == \"__main__\":\n","    main()\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5246715,"sourceId":8739218,"sourceType":"datasetVersion"},{"datasetId":5270304,"sourceId":8770321,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
